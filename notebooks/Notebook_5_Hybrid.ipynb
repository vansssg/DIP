{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 5: Hybrid Image Restoration\n",
        "\n",
        "This notebook implements a hybrid approach combining PDE-based restoration with CNN refinement. The pipeline: PDE output → CNN refinement → final restored image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sys.path.append('..')\n",
        "from utils import load_image, save_image, ensure_dir, get_image_files\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define U-Net architecture (same as Notebook_4)\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"Double convolution block.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"U-Net architecture for image inpainting.\"\"\"\n",
        "    def __init__(self, in_channels=4, out_channels=3):\n",
        "        super(UNet, self).__init__()\n",
        "        \n",
        "        # Encoder (downsampling)\n",
        "        self.enc1 = DoubleConv(in_channels, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = DoubleConv(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.enc3 = DoubleConv(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.enc4 = DoubleConv(256, 512)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "        \n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(512, 1024)\n",
        "        \n",
        "        # Decoder (upsampling)\n",
        "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.dec4 = DoubleConv(1024, 512)\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.dec3 = DoubleConv(512, 256)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = DoubleConv(256, 128)\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = DoubleConv(128, 64)\n",
        "        \n",
        "        # Output layer\n",
        "        self.final = nn.Conv2d(64, out_channels, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        e3 = self.enc3(self.pool2(e2))\n",
        "        e4 = self.enc4(self.pool3(e3))\n",
        "        \n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(self.pool4(e4))\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        d4 = self.up4(b)\n",
        "        d4 = torch.cat([d4, e4], dim=1)\n",
        "        d4 = self.dec4(d4)\n",
        "        \n",
        "        d3 = self.up3(d4)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "        \n",
        "        d2 = self.up2(d3)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "        \n",
        "        d1 = self.up1(d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "        \n",
        "        # Output\n",
        "        out = self.final(d1)\n",
        "        return torch.sigmoid(out)  # Output in [0, 1] range\n",
        "\n",
        "print(\"U-Net model defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load PDE Results and Trained CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "PDE_RESULTS_DIR = Path('../methods/PDE/results')\n",
        "MASKS_DIR = Path('../data/masks')\n",
        "MODEL_PATH = Path('../models/unet_inpainting.pth')\n",
        "RESULTS_DIR = Path('../methods/Hybrid/results')\n",
        "\n",
        "ensure_dir(RESULTS_DIR)\n",
        "\n",
        "# Load trained U-Net model\n",
        "print(\"Loading trained U-Net model...\")\n",
        "model = UNet(in_channels=4, out_channels=3).to(device)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Get PDE result files\n",
        "pde_files = get_image_files(PDE_RESULTS_DIR)\n",
        "print(f\"Found {len(pde_files)} PDE results to refine\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hybrid Pipeline: PDE + CNN Refinement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def refine_with_cnn(model, pde_result, mask, device):\n",
        "    \"\"\"\n",
        "    Refine PDE result using CNN.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained U-Net model\n",
        "        pde_result: PDE restoration output (RGB)\n",
        "        mask: Binary mask (255 for damaged regions)\n",
        "        device: Device for computation\n",
        "        \n",
        "    Returns:\n",
        "        Refined image\n",
        "    \"\"\"\n",
        "    h, w = pde_result.shape[:2]\n",
        "    \n",
        "    # Resize for model input\n",
        "    pde_resized = cv2.resize(pde_result, (256, 256))\n",
        "    mask_resized = cv2.resize(mask, (256, 256))\n",
        "    \n",
        "    # Normalize\n",
        "    pde_norm = pde_resized.astype(np.float32) / 255.0\n",
        "    mask_norm = mask_resized.astype(np.float32) / 255.0\n",
        "    \n",
        "    # Convert to tensor\n",
        "    pde_tensor = torch.from_numpy(pde_norm).permute(2, 0, 1).unsqueeze(0)\n",
        "    mask_tensor = torch.from_numpy(mask_norm).unsqueeze(0).unsqueeze(0)\n",
        "    input_tensor = torch.cat([pde_tensor, mask_tensor], dim=1).to(device)\n",
        "    \n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        output = output.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
        "        output = (output * 255).astype(np.uint8)\n",
        "    \n",
        "    # Resize back to original size\n",
        "    output = cv2.resize(output, (w, h))\n",
        "    \n",
        "    # Blend: use CNN output in masked regions, keep PDE result elsewhere\n",
        "    mask_bool = (mask == 255)[:, :, np.newaxis]\n",
        "    refined = np.where(mask_bool, output, pde_result)\n",
        "    \n",
        "    return refined\n",
        "\n",
        "print(\"Refinement function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test on Sample Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on sample\n",
        "if pde_files:\n",
        "    sample_pde = load_image(pde_files[0])\n",
        "    sample_mask = cv2.imread(str(MASKS_DIR / f\"{pde_files[0].stem}.png\"), cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    print(f\"Testing on: {pde_files[0].name}\")\n",
        "    \n",
        "    # Refine with CNN\n",
        "    refined = refine_with_cnn(model, sample_pde, sample_mask, device)\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    axes[0].imshow(sample_pde)\n",
        "    axes[0].set_title('PDE Result')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(refined)\n",
        "    axes[1].set_title('Hybrid (PDE + CNN)')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    diff = np.abs(refined.astype(float) - sample_pde.astype(float))\n",
        "    axes[2].imshow(diff.astype(np.uint8))\n",
        "    axes[2].set_title('Difference (Hybrid - PDE)')\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Sample test complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Process All Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all PDE results\n",
        "print(f\"Processing {len(pde_files)} images with hybrid pipeline...\")\n",
        "\n",
        "for pde_path in tqdm(pde_files, desc=\"Hybrid Refinement\"):\n",
        "    # Load PDE result and mask\n",
        "    pde_result = load_image(pde_path)\n",
        "    mask = cv2.imread(str(MASKS_DIR / f\"{pde_path.stem}.png\"), cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    # Refine with CNN\n",
        "    hybrid_result = refine_with_cnn(model, pde_result, mask, device)\n",
        "    \n",
        "    # Save result\n",
        "    save_image(hybrid_result, RESULTS_DIR / f\"{pde_path.stem}.png\")\n",
        "\n",
        "print(f\"\\n✓ All {len(pde_files)} images processed and saved to {RESULTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary\n",
        "\n",
        "Hybrid restoration complete! All restored images saved to `methods/Hybrid/results/`.\n",
        "\n",
        "**Pipeline:**\n",
        "1. PDE-based inpainting (from Notebook_2)\n",
        "2. CNN refinement using trained U-Net (from Notebook_4)\n",
        "3. Blending: CNN output in masked regions, PDE result elsewhere\n",
        "\n",
        "The hybrid approach combines the smoothness of PDE methods with the detail-preserving capabilities of deep learning.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
