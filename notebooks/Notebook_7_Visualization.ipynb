{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 7: Results Visualization\n",
        "\n",
        "This notebook creates comprehensive visualizations including side-by-side comparisons, metric charts, and analysis reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "sys.path.append('..')\n",
        "from utils import load_image, get_image_files, visualize_comparison\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Evaluation Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load evaluation results\n",
        "results_df = pd.read_csv('../evaluation_results.csv')\n",
        "summary_df = pd.read_csv('../evaluation_summary.csv')\n",
        "\n",
        "print(f\"Loaded results for {len(results_df)} images\")\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(summary_df)\n",
        "\n",
        "# Paths\n",
        "GROUND_TRUTH_DIR = Path('../data/ground_truth')\n",
        "DAMAGED_DIR = Path('../data/damaged')\n",
        "METHODS = {\n",
        "    'PDE': Path('../methods/PDE/results'),\n",
        "    'Patch': Path('../methods/Patch/results'),\n",
        "    'Deep': Path('../methods/Deep/results'),\n",
        "    'Hybrid': Path('../methods/Hybrid/results')\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Metric Comparison Charts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for plotting\n",
        "metrics_data = []\n",
        "for method in ['PDE', 'Patch', 'Deep', 'Hybrid']:\n",
        "    metrics_data.append({\n",
        "        'Method': method,\n",
        "        'PSNR': results_df[f'{method}_PSNR'].mean(),\n",
        "        'SSIM': results_df[f'{method}_SSIM'].mean(),\n",
        "        'LPIPS': results_df[f'{method}_LPIPS'].mean(),\n",
        "        'EdgeAcc': results_df[f'{method}_EdgeAcc'].mean()\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "# Create comparison charts\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# PSNR\n",
        "axes[0, 0].bar(metrics_df['Method'], metrics_df['PSNR'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[0, 0].set_title('PSNR Comparison (Higher is Better)', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('PSNR (dB)')\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# SSIM\n",
        "axes[0, 1].bar(metrics_df['Method'], metrics_df['SSIM'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[0, 1].set_title('SSIM Comparison (Higher is Better)', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('SSIM')\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# LPIPS\n",
        "axes[1, 0].bar(metrics_df['Method'], metrics_df['LPIPS'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[1, 0].set_title('LPIPS Comparison (Lower is Better)', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('LPIPS')\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Edge Accuracy\n",
        "axes[1, 1].bar(metrics_df['Method'], metrics_df['EdgeAcc'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[1, 1].set_title('Edge Accuracy Comparison (Higher is Better)', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Edge Accuracy')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../metric_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Metric comparison charts saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Box Plots for Metric Distributions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for box plots\n",
        "box_data = []\n",
        "for method in ['PDE', 'Patch', 'Deep', 'Hybrid']:\n",
        "    for metric in ['PSNR', 'SSIM', 'LPIPS', 'EdgeAcc']:\n",
        "        values = results_df[f'{method}_{metric}'].dropna()\n",
        "        for val in values:\n",
        "            box_data.append({\n",
        "                'Method': method,\n",
        "                'Metric': metric,\n",
        "                'Value': val\n",
        "            })\n",
        "\n",
        "box_df = pd.DataFrame(box_data)\n",
        "\n",
        "# Create box plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics_to_plot = ['PSNR', 'SSIM', 'LPIPS', 'EdgeAcc']\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    metric_data = box_df[box_df['Metric'] == metric]\n",
        "    sns.boxplot(data=metric_data, x='Method', y='Value', ax=ax)\n",
        "    ax.set_title(f'{metric} Distribution', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel(metric)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../metric_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Box plots saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Side-by-Side Visual Comparisons\n",
        "\n",
        "Create visual comparisons for best, worst, and random sample images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best and worst cases (based on average PSNR across all methods)\n",
        "results_df['avg_PSNR'] = results_df[['PDE_PSNR', 'Patch_PSNR', 'Deep_PSNR', 'Hybrid_PSNR']].mean(axis=1)\n",
        "best_idx = results_df['avg_PSNR'].idxmax()\n",
        "worst_idx = results_df['avg_PSNR'].idxmin()\n",
        "random_idx = np.random.randint(0, len(results_df))\n",
        "\n",
        "sample_indices = [best_idx, worst_idx, random_idx]\n",
        "sample_names = ['Best Case', 'Worst Case', 'Random Sample']\n",
        "\n",
        "for sample_idx, sample_name in zip(sample_indices, sample_names):\n",
        "    img_name = results_df.iloc[sample_idx]['image_name']\n",
        "    \n",
        "    # Load all images\n",
        "    gt = load_image(GROUND_TRUTH_DIR / f\"{img_name}.png\")\n",
        "    damaged = load_image(DAMAGED_DIR / f\"{img_name}.png\")\n",
        "    \n",
        "    method_results = {}\n",
        "    for method_name, method_dir in METHODS.items():\n",
        "        method_path = method_dir / f\"{img_name}.png\"\n",
        "        if method_path.exists():\n",
        "            method_results[method_name] = load_image(method_path)\n",
        "    \n",
        "    # Create comparison figure\n",
        "    num_methods = len(method_results)\n",
        "    fig, axes = plt.subplots(2, 3 + num_methods, figsize=(20, 8))\n",
        "    \n",
        "    # First row: Ground truth, damaged, and all methods\n",
        "    axes[0, 0].imshow(gt)\n",
        "    axes[0, 0].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].axis('off')\n",
        "    \n",
        "    axes[0, 1].imshow(damaged)\n",
        "    axes[0, 1].set_title('Damaged', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].axis('off')\n",
        "    \n",
        "    for idx, (method_name, method_img) in enumerate(method_results.items(), start=2):\n",
        "        axes[0, idx].imshow(method_img)\n",
        "        psnr_val = results_df.iloc[sample_idx][f'{method_name}_PSNR']\n",
        "        ssim_val = results_df.iloc[sample_idx][f'{method_name}_SSIM']\n",
        "        axes[0, idx].set_title(f'{method_name}\\nPSNR: {psnr_val:.2f}, SSIM: {ssim_val:.3f}', \n",
        "                               fontsize=10, fontweight='bold')\n",
        "        axes[0, idx].axis('off')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for idx in range(2 + num_methods, 3 + num_methods):\n",
        "        axes[0, idx].axis('off')\n",
        "    \n",
        "    # Second row: Difference maps\n",
        "    axes[1, 0].axis('off')\n",
        "    axes[1, 1].axis('off')\n",
        "    \n",
        "    for idx, (method_name, method_img) in enumerate(method_results.items(), start=2):\n",
        "        diff = np.abs(gt.astype(float) - method_img.astype(float))\n",
        "        diff_normalized = (diff / diff.max() * 255).astype(np.uint8) if diff.max() > 0 else diff.astype(np.uint8)\n",
        "        axes[1, idx].imshow(diff_normalized)\n",
        "        axes[1, idx].set_title(f'{method_name} Difference', fontsize=10)\n",
        "        axes[1, idx].axis('off')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for idx in range(2 + num_methods, 3 + num_methods):\n",
        "        axes[1, idx].axis('off')\n",
        "    \n",
        "    plt.suptitle(f'{sample_name}: {img_name}', fontsize=16, fontweight='bold', y=0.98)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'../comparison_{sample_name.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Visual comparisons saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary Report\n",
        "\n",
        "Generate a text summary of findings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "report = f\"\"\"\n",
        "# Mural Restoration Evaluation Report\n",
        "\n",
        "## Dataset\n",
        "- Total images evaluated: {len(results_df)}\n",
        "- Methods compared: PDE, Patch, Deep Learning (U-Net), Hybrid\n",
        "\n",
        "## Overall Performance Summary\n",
        "\n",
        "### PSNR (Peak Signal-to-Noise Ratio) - Higher is Better\n",
        "\"\"\"\n",
        "for method in ['PDE', 'Patch', 'Deep', 'Hybrid']:\n",
        "    mean_psnr = results_df[f'{method}_PSNR'].mean()\n",
        "    std_psnr = results_df[f'{method}_PSNR'].std()\n",
        "    report += f\"- {method}: {mean_psnr:.4f} ± {std_psnr:.4f} dB\\n\"\n",
        "\n",
        "report += \"\\n### SSIM (Structural Similarity Index) - Higher is Better\\n\"\n",
        "for method in ['PDE', 'Patch', 'Deep', 'Hybrid']:\n",
        "    mean_ssim = results_df[f'{method}_SSIM'].mean()\n",
        "    std_ssim = results_df[f'{method}_SSIM'].std()\n",
        "    report += f\"- {method}: {mean_ssim:.4f} ± {std_ssim:.4f}\\n\"\n",
        "\n",
        "report += \"\\n### LPIPS (Learned Perceptual Image Patch Similarity) - Lower is Better\\n\"\n",
        "for method in ['PDE', 'Patch', 'Deep', 'Hybrid']:\n",
        "    mean_lpips = results_df[f'{method}_LPIPS'].mean()\n",
        "    std_lpips = results_df[f'{method}_LPIPS'].std()\n",
        "    report += f\"- {method}: {mean_lpips:.4f} ± {std_lpips:.4f}\\n\"\n",
        "\n",
        "report += \"\\n### Edge Accuracy - Higher is Better\\n\"\n",
        "for method in ['PDE', 'Patch', 'Deep', 'Hybrid']:\n",
        "    mean_edge = results_df[f'{method}_EdgeAcc'].mean()\n",
        "    std_edge = results_df[f'{method}_EdgeAcc'].std()\n",
        "    report += f\"- {method}: {mean_edge:.4f} ± {std_edge:.4f}\\n\"\n",
        "\n",
        "# Find best method for each metric\n",
        "best_psnr = summary_df.loc[summary_df['PSNR_mean'].idxmax(), 'Method']\n",
        "best_ssim = summary_df.loc[summary_df['SSIM_mean'].idxmax(), 'Method']\n",
        "best_lpips = summary_df.loc[summary_df['LPIPS_mean'].idxmin(), 'Method']\n",
        "best_edge = summary_df.loc[summary_df['EdgeAcc_mean'].idxmax(), 'Method']\n",
        "\n",
        "report += f\"\"\"\n",
        "## Best Methods by Metric\n",
        "- Best PSNR: {best_psnr}\n",
        "- Best SSIM: {best_ssim}\n",
        "- Best LPIPS (lowest): {best_lpips}\n",
        "- Best Edge Accuracy: {best_edge}\n",
        "\n",
        "## Conclusions\n",
        "The evaluation shows the relative performance of different restoration methods on the mural dataset.\n",
        "Visual comparisons and detailed metrics are available in the generated charts and CSV files.\n",
        "\"\"\"\n",
        "\n",
        "# Save report\n",
        "with open('../evaluation_report.md', 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(report)\n",
        "print(\"\\n✓ Report saved to evaluation_report.md\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary\n",
        "\n",
        "Visualization complete! Generated outputs:\n",
        "- Metric comparison charts (`metric_comparison.png`)\n",
        "- Metric distribution box plots (`metric_distributions.png`)\n",
        "- Side-by-side visual comparisons for best/worst/random cases\n",
        "- Evaluation report (`evaluation_report.md`)\n",
        "\n",
        "All visualizations provide comprehensive analysis of the restoration methods' performance.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
