{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 6: Quantitative Evaluation\n",
        "\n",
        "This notebook computes evaluation metrics (PSNR, SSIM, LPIPS, Edge Accuracy) for all restoration methods and stores results in a CSV file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import lpips\n",
        "\n",
        "sys.path.append('..')\n",
        "from utils import (\n",
        "    load_image, get_image_files, \n",
        "    calculate_psnr, calculate_ssim, \n",
        "    calculate_lpips, calculate_edge_accuracy\n",
        ")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Paths and Initialize LPIPS Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "GROUND_TRUTH_DIR = Path('../data/ground_truth')\n",
        "METHODS = {\n",
        "    'PDE': Path('../methods/PDE/results'),\n",
        "    'Patch': Path('../methods/Patch/results'),\n",
        "    'Deep': Path('../methods/Deep/results'),\n",
        "    'Hybrid': Path('../methods/Hybrid/results')\n",
        "}\n",
        "\n",
        "# Initialize LPIPS model (only once)\n",
        "print(\"Initializing LPIPS model...\")\n",
        "lpips_model = lpips.LPIPS(net='alex')\n",
        "print(\"LPIPS model ready!\")\n",
        "\n",
        "# Get all ground truth files\n",
        "gt_files = get_image_files(GROUND_TRUTH_DIR)\n",
        "print(f\"Found {len(gt_files)} ground truth images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Compute Metrics for All Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize results list\n",
        "results = []\n",
        "\n",
        "print(\"Computing metrics for all methods...\")\n",
        "\n",
        "for gt_path in tqdm(gt_files, desc=\"Evaluating\"):\n",
        "    gt_img = load_image(gt_path)\n",
        "    base_name = gt_path.stem\n",
        "    \n",
        "    result_row = {'image_name': base_name}\n",
        "    \n",
        "    # Evaluate each method\n",
        "    for method_name, method_dir in METHODS.items():\n",
        "        method_result_path = method_dir / f\"{base_name}.png\"\n",
        "        \n",
        "        if method_result_path.exists():\n",
        "            method_result = load_image(method_result_path)\n",
        "            \n",
        "            # Compute metrics\n",
        "            psnr_val = calculate_psnr(gt_img, method_result)\n",
        "            ssim_val = calculate_ssim(gt_img, method_result)\n",
        "            lpips_val = calculate_lpips(gt_img, method_result, lpips_model)\n",
        "            edge_acc = calculate_edge_accuracy(gt_img, method_result)\n",
        "            \n",
        "            result_row[f'{method_name}_PSNR'] = psnr_val\n",
        "            result_row[f'{method_name}_SSIM'] = ssim_val\n",
        "            result_row[f'{method_name}_LPIPS'] = lpips_val\n",
        "            result_row[f'{method_name}_EdgeAcc'] = edge_acc\n",
        "        else:\n",
        "            # If result doesn't exist, set to NaN\n",
        "            result_row[f'{method_name}_PSNR'] = np.nan\n",
        "            result_row[f'{method_name}_SSIM'] = np.nan\n",
        "            result_row[f'{method_name}_LPIPS'] = np.nan\n",
        "            result_row[f'{method_name}_EdgeAcc'] = np.nan\n",
        "    \n",
        "    results.append(result_row)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "print(f\"\\nComputed metrics for {len(df)} images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute summary statistics for each method\n",
        "summary_stats = []\n",
        "\n",
        "for method_name in METHODS.keys():\n",
        "    method_stats = {\n",
        "        'Method': method_name,\n",
        "        'PSNR_mean': df[f'{method_name}_PSNR'].mean(),\n",
        "        'PSNR_std': df[f'{method_name}_PSNR'].std(),\n",
        "        'SSIM_mean': df[f'{method_name}_SSIM'].mean(),\n",
        "        'SSIM_std': df[f'{method_name}_SSIM'].std(),\n",
        "        'LPIPS_mean': df[f'{method_name}_LPIPS'].mean(),\n",
        "        'LPIPS_std': df[f'{method_name}_LPIPS'].std(),\n",
        "        'EdgeAcc_mean': df[f'{method_name}_EdgeAcc'].mean(),\n",
        "        'EdgeAcc_std': df[f'{method_name}_EdgeAcc'].std()\n",
        "    }\n",
        "    summary_stats.append(method_stats)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_stats)\n",
        "\n",
        "print(\"\\n=== Summary Statistics ===\")\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# Display best method for each metric\n",
        "print(\"\\n=== Best Methods ===\")\n",
        "print(f\"Best PSNR: {summary_df.loc[summary_df['PSNR_mean'].idxmax(), 'Method']} ({summary_df['PSNR_mean'].max():.4f})\")\n",
        "print(f\"Best SSIM: {summary_df.loc[summary_df['SSIM_mean'].idxmax(), 'Method']} ({summary_df['SSIM_mean'].max():.4f})\")\n",
        "print(f\"Best LPIPS (lowest): {summary_df.loc[summary_df['LPIPS_mean'].idxmin(), 'Method']} ({summary_df['LPIPS_mean'].min():.4f})\")\n",
        "print(f\"Best Edge Accuracy: {summary_df.loc[summary_df['EdgeAcc_mean'].idxmax(), 'Method']} ({summary_df['EdgeAcc_mean'].max():.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Results to CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results\n",
        "results_csv_path = Path('../evaluation_results.csv')\n",
        "df.to_csv(results_csv_path, index=False)\n",
        "print(f\"Detailed results saved to: {results_csv_path}\")\n",
        "\n",
        "# Save summary statistics\n",
        "summary_csv_path = Path('../evaluation_summary.csv')\n",
        "summary_df.to_csv(summary_csv_path, index=False)\n",
        "print(f\"Summary statistics saved to: {summary_csv_path}\")\n",
        "\n",
        "print(\"\\nâœ“ Evaluation complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
