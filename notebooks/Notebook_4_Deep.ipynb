{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 4: Deep Learning (U-Net) Image Restoration\n",
        "\n",
        "This notebook implements a U-Net architecture for image inpainting using PyTorch. The model learns to restore damaged regions by training on damaged images and masks as input, with ground truth as target.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "sys.path.append('..')\n",
        "from utils import load_image, save_image, ensure_dir, get_image_files\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. U-Net Architecture Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"Double convolution block.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"U-Net architecture for image inpainting.\"\"\"\n",
        "    def __init__(self, in_channels=4, out_channels=3):\n",
        "        super(UNet, self).__init__()\n",
        "        \n",
        "        # Encoder (downsampling)\n",
        "        self.enc1 = DoubleConv(in_channels, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = DoubleConv(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.enc3 = DoubleConv(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.enc4 = DoubleConv(256, 512)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "        \n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(512, 1024)\n",
        "        \n",
        "        # Decoder (upsampling)\n",
        "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.dec4 = DoubleConv(1024, 512)\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.dec3 = DoubleConv(512, 256)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec2 = DoubleConv(256, 128)\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec1 = DoubleConv(128, 64)\n",
        "        \n",
        "        # Output layer\n",
        "        self.final = nn.Conv2d(64, out_channels, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool1(e1))\n",
        "        e3 = self.enc3(self.pool2(e2))\n",
        "        e4 = self.enc4(self.pool3(e3))\n",
        "        \n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(self.pool4(e4))\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        d4 = self.up4(b)\n",
        "        d4 = torch.cat([d4, e4], dim=1)\n",
        "        d4 = self.dec4(d4)\n",
        "        \n",
        "        d3 = self.up3(d4)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "        \n",
        "        d2 = self.up2(d3)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "        \n",
        "        d1 = self.up1(d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "        \n",
        "        # Output\n",
        "        out = self.final(d1)\n",
        "        return torch.sigmoid(out)  # Output in [0, 1] range\n",
        "\n",
        "print(\"U-Net model defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InpaintingDataset(Dataset):\n",
        "    \"\"\"Dataset for image inpainting.\"\"\"\n",
        "    def __init__(self, damaged_dir, mask_dir, gt_dir, transform=None, target_size=(256, 256)):\n",
        "        self.damaged_files = sorted(get_image_files(damaged_dir))\n",
        "        self.mask_dir = mask_dir\n",
        "        self.gt_dir = gt_dir\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.damaged_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Load damaged image\n",
        "        damaged = load_image(self.damaged_files[idx])\n",
        "        \n",
        "        # Load mask\n",
        "        mask_path = Path(self.mask_dir) / f\"{self.damaged_files[idx].stem}.png\"\n",
        "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
        "        \n",
        "        # Load ground truth\n",
        "        gt_path = Path(self.gt_dir) / f\"{self.damaged_files[idx].stem}.png\"\n",
        "        gt = load_image(gt_path)\n",
        "        \n",
        "        # Resize if needed\n",
        "        if self.target_size:\n",
        "            damaged = cv2.resize(damaged, self.target_size)\n",
        "            mask = cv2.resize(mask, self.target_size)\n",
        "            gt = cv2.resize(gt, self.target_size)\n",
        "        \n",
        "        # Normalize to [0, 1]\n",
        "        damaged = damaged.astype(np.float32) / 255.0\n",
        "        mask = mask.astype(np.float32) / 255.0\n",
        "        gt = gt.astype(np.float32) / 255.0\n",
        "        \n",
        "        # Convert to tensors: [C, H, W]\n",
        "        damaged_tensor = torch.from_numpy(damaged).permute(2, 0, 1)\n",
        "        mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
        "        gt_tensor = torch.from_numpy(gt).permute(2, 0, 1)\n",
        "        \n",
        "        # Concatenate damaged image and mask as input (4 channels)\n",
        "        input_tensor = torch.cat([damaged_tensor, mask_tensor], dim=0)\n",
        "        \n",
        "        return input_tensor, gt_tensor\n",
        "\n",
        "print(\"Dataset class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Loss Function (L1 + Perceptual Loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VGGPerceptualLoss(nn.Module):\n",
        "    \"\"\"Perceptual loss using VGG features.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(VGGPerceptualLoss, self).__init__()\n",
        "        vgg = torchvision.models.vgg16(pretrained=True).features\n",
        "        self.feature_layers = nn.ModuleList([\n",
        "            vgg[:4],   # relu1_2\n",
        "            vgg[4:9],  # relu2_2\n",
        "            vgg[9:16], # relu3_3\n",
        "            vgg[16:23] # relu4_3\n",
        "        ])\n",
        "        for param in self.feature_layers.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        loss = 0\n",
        "        for layer in self.feature_layers:\n",
        "            pred = layer(pred)\n",
        "            target = layer(target)\n",
        "            loss += F.mse_loss(pred, target)\n",
        "        return loss\n",
        "\n",
        "\n",
        "def combined_loss(pred, target, perceptual_loss_fn=None, lambda_l1=1.0, lambda_perceptual=0.1):\n",
        "    \"\"\"Combined L1 and perceptual loss.\"\"\"\n",
        "    l1_loss = F.l1_loss(pred, target)\n",
        "    \n",
        "    if perceptual_loss_fn is not None:\n",
        "        # Normalize for VGG (ImageNet stats)\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(pred.device)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(pred.device)\n",
        "        pred_norm = (pred - mean) / std\n",
        "        target_norm = (target - mean) / std\n",
        "        perc_loss = perceptual_loss_fn(pred_norm, target_norm)\n",
        "    else:\n",
        "        perc_loss = 0\n",
        "    \n",
        "    return lambda_l1 * l1_loss + lambda_perceptual * perc_loss\n",
        "\n",
        "print(\"Loss functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "DAMAGED_DIR = Path('../data/damaged')\n",
        "MASKS_DIR = Path('../data/masks')\n",
        "GROUND_TRUTH_DIR = Path('../data/ground_truth')\n",
        "RESULTS_DIR = Path('../models')\n",
        "MODEL_SAVE_PATH = RESULTS_DIR / 'unet_inpainting.pth'\n",
        "\n",
        "ensure_dir(RESULTS_DIR)\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 10  # Adjust based on dataset size\n",
        "TARGET_SIZE = (256, 256)  # Resize for training\n",
        "\n",
        "# Create dataset\n",
        "print(\"Loading dataset...\")\n",
        "full_dataset = InpaintingDataset(DAMAGED_DIR, MASKS_DIR, GROUND_TRUTH_DIR, target_size=TARGET_SIZE)\n",
        "\n",
        "# Split into train/val (80/20)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Val samples: {len(val_dataset)}\")\n",
        "\n",
        "# Initialize model\n",
        "model = UNet(in_channels=4, out_channels=3).to(device)\n",
        "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Loss and optimizer\n",
        "perceptual_loss_fn = VGGPerceptualLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "print(\"Training setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = combined_loss(outputs, targets, perceptual_loss_fn, lambda_l1=1.0, lambda_perceptual=0.1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = combined_loss(outputs, targets, perceptual_loss_fn, lambda_l1=1.0, lambda_perceptual=0.1)\n",
        "            val_loss += loss.item()\n",
        "    \n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    \n",
        "    scheduler.step(avg_val_loss)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        print(f\"  -> Saved best model (val_loss: {best_val_loss:.4f})\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test on All Images\n",
        "\n",
        "Load the trained model and process all images at full resolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "RESULTS_DIR = Path('../methods/Deep/results')\n",
        "ensure_dir(RESULTS_DIR)\n",
        "\n",
        "# Get all test images (use full dataset)\n",
        "test_files = get_image_files(DAMAGED_DIR)\n",
        "\n",
        "print(f\"Processing {len(test_files)} images for testing...\")\n",
        "\n",
        "def process_image(model, damaged_img, mask, device):\n",
        "    \"\"\"Process a single image with the model.\"\"\"\n",
        "    h, w = damaged_img.shape[:2]\n",
        "    \n",
        "    # Resize for model input\n",
        "    damaged_resized = cv2.resize(damaged_img, (256, 256))\n",
        "    mask_resized = cv2.resize(mask, (256, 256))\n",
        "    \n",
        "    # Normalize\n",
        "    damaged_norm = damaged_resized.astype(np.float32) / 255.0\n",
        "    mask_norm = mask_resized.astype(np.float32) / 255.0\n",
        "    \n",
        "    # Convert to tensor\n",
        "    damaged_tensor = torch.from_numpy(damaged_norm).permute(2, 0, 1).unsqueeze(0)\n",
        "    mask_tensor = torch.from_numpy(mask_norm).unsqueeze(0).unsqueeze(0)\n",
        "    input_tensor = torch.cat([damaged_tensor, mask_tensor], dim=1).to(device)\n",
        "    \n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        output = output.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
        "        output = (output * 255).astype(np.uint8)\n",
        "    \n",
        "    # Resize back to original size\n",
        "    output = cv2.resize(output, (w, h))\n",
        "    \n",
        "    return output\n",
        "\n",
        "# Process all images\n",
        "for img_path in tqdm(test_files, desc=\"Deep Learning Inference\"):\n",
        "    damaged_img = load_image(img_path)\n",
        "    mask = cv2.imread(str(MASKS_DIR / f\"{img_path.stem}.png\"), cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    restored = process_image(model, damaged_img, mask, device)\n",
        "    \n",
        "    save_image(restored, RESULTS_DIR / f\"{img_path.stem}.png\")\n",
        "\n",
        "print(f\"\\nâœ“ All {len(test_files)} images processed and saved to {RESULTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "Deep learning restoration complete! All restored images saved to `methods/Deep/results/`.\n",
        "\n",
        "**Model Architecture:** U-Net with skip connections\n",
        "**Training:** L1 loss + Perceptual loss (VGG features)\n",
        "**Input:** 4 channels (RGB damaged image + mask)\n",
        "**Output:** 3 channels (RGB restored image)\n",
        "\n",
        "The deep learning method learns complex patterns and textures from the training data, making it effective for various types of damage.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
